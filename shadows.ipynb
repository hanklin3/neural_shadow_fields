{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09cfae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch \n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "from datasets import dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47b372d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    dataset = dataset_dict['blender_shadows']\n",
    "    kwargs = {'root_dir': '../datasets/volumetric/results_500/',\n",
    "              'img_wh': (400,400), \n",
    "              'hparams': None,\n",
    "              }\n",
    "    return dataset(split='val', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "447bd278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Image size: (400, 400)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e54ee4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(train_dataset, shuffle=False, num_workers=4, batch_size=1, pin_memory=True)\n",
    "loader = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52289706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rays': tensor([[[-47.8855,  30.9148,  98.1950,  ...,  -0.7324,   1.0000, 150.0000],\n",
       "          [-47.8855,  30.9148,  98.1950,  ...,  -0.7325,   1.0000, 150.0000],\n",
       "          [-47.8855,  30.9148,  98.1950,  ...,  -0.7327,   1.0000, 150.0000],\n",
       "          ...,\n",
       "          [-47.8855,  30.9148,  98.1950,  ...,  -0.9296,   1.0000, 150.0000],\n",
       "          [-47.8855,  30.9148,  98.1950,  ...,  -0.9294,   1.0000, 150.0000],\n",
       "          [-47.8855,  30.9148,  98.1950,  ...,  -0.9293,   1.0000, 150.0000]]]),\n",
       " 'rgbs': tensor([[[0.9137, 0.7490, 0.9647],\n",
       "          [0.9412, 0.7725, 0.9412],\n",
       "          [0.9294, 0.7765, 0.9333],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000]]]),\n",
       " 'c2w': tensor([[[-5.4239e-01,  7.2659e-01, -4.2176e-01, -4.7886e+01],\n",
       "          [-8.4013e-01, -4.6909e-01,  2.7228e-01,  3.0915e+01],\n",
       "          [ 1.4901e-08,  5.0201e-01,  8.6486e-01,  9.8195e+01]]]),\n",
       " 'valid_mask': tensor([[ True,  True,  True,  ..., False, False, False]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(loader)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8dc651c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "T = transforms.ToTensor()\n",
    "\n",
    "depth = Image.open(\"r_0_depth_0093.png\")\n",
    "depth = depth.resize((400,400))\n",
    "depth = T(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f516742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camera import Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01d21b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  0.0000e+00, -4.0000e+02],\n",
       "        [ 0.0000e+00, -1.0000e+00,  4.0000e+02],\n",
       "        [ 0.0000e+00,  0.0000e+00, -8.1695e+03]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fov = 5.60622567406272 # 0.0978470966219902 rad \n",
    "cam = Camera(fov, (800,800))\n",
    "cam.camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dc858fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.set_pose_using_blender_matrix(c2w[:3, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729837c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8.8843e-01,  3.4820e-01, -2.9378e+03],\n",
       "         [ 4.5886e-01, -6.5693e-01,  4.9662e+03],\n",
       "         [ 1.1830e-02, -6.6873e-01, -5.8109e+03]]),\n",
       " tensor([ 200., -400.,  500.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam.camera, cam.eye_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137cfc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['camera_angle_x', 'light_camera_angle_x', 'light_camera_transform_matrix', 'frames'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file = \"../datasets/volumetric/results_500_v2/transforms.json\"\n",
    "with open(file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3979135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0978470966219902,\n",
       " array([[ 8.88431907e-01, -3.48196030e-01,  2.99056917e-01,\n",
       "          2.00000000e+02],\n",
       "        [ 4.58855867e-01,  6.56932235e-01, -5.98198533e-01,\n",
       "         -4.00000000e+02],\n",
       "        [ 1.18302405e-02,  6.68729782e-01,  7.43453681e-01,\n",
       "          5.00000000e+02]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fov = data['light_camera_angle_x']\n",
    "c2w = np.array(data['light_camera_transform_matrix'])\n",
    "fov, c2w[:3, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fd57418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.88431907e-01, -3.48196030e-01,  2.99056917e-01,\n",
       "         2.00000000e+02],\n",
       "       [ 4.58855867e-01,  6.56932235e-01, -5.98198533e-01,\n",
       "        -4.00000000e+02],\n",
       "       [ 1.18302405e-02,  6.68729782e-01,  7.43453681e-01,\n",
       "         5.00000000e+02]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2w[:3, :4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nerf_pl]",
   "language": "python",
   "name": "conda-env-.conda-nerf_pl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
